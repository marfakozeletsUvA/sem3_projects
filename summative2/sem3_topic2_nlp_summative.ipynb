{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0-markdown",
   "metadata": {},
   "source": [
    "# Semester 3 Coding Portfolio Topic 2 Summative:\n",
    "# Natural Language Processing\n",
    "\n",
    "In this notebook, you are asked to do original work with little guidance, based on the skills you learned in the formative part (as well as lectures and workshops).\n",
    "This section is graded not just on passing automated tests, but also on quality, originality, and effort (see assessment criteria in the assignment description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Please enter your student number here\n",
    "STUDENT_NUMBER = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2-markdown",
   "metadata": {},
   "source": [
    "# SUMMATIVE ASSESSMENT\n",
    "\n",
    "For this summative assignment, we ask you to find a dataset from an internet source of choice. You will then create an NLP pipeline including preprocessing, NLP analysis, and classification.\n",
    "Your anlysis for this notebook should have two parts: An initial NLP analysis (as done in formative notebook 1), and a classification of these results (as done in formative notebook 2). Chose one method for each of these two steps.\n",
    "\n",
    "You should chose ONE of the following:\n",
    " - Sentiment Analysis\n",
    " - LDA\n",
    " - BertTopic\n",
    "\n",
    "You should ALSO chose ONE of the following:\n",
    " - Decision Tree / Random Forest\n",
    " - LLM-based text classification\n",
    "\n",
    "\n",
    "The general assessment criteria for all summative assignments are mentioned in the assignment description on Canvas. Each notebook also has a few specific criteria we look for; make sure you fulfil them in your approach to this assignment.\n",
    "In general, make sure this notebook represents a complete project: Write an explanation of what you are hoping to achieve with your analysis, document your code well, and present results in a comprehensive way.\n",
    "The assessment criteria for this notebook vary slightly depending on which methods you chose to implement:\n",
    "\n",
    "## Sentiment Analysis\n",
    " - Selected an appropriate dataset and prepared it for analysis, including cleaning and formatting the data.\n",
    " - Effectively pre-processed the text data, including steps such as tokenization, stopword removal, lemmatization, and handling special characters.\n",
    " - Selected an appropriate sentiment analysis model or algorithm for their dataset and correctly implemented the sentiment analysis model, ensuring it is properly trained and tested.\n",
    " - Provided a clear and insightful interpretation of the sentiment analysis results, explaining the significance and implications of their findings.\n",
    "\n",
    "## LDA \n",
    " - Selected an appropriate dataset and prepared it for analysis, including cleaning and formatting the data.\n",
    " - Correctly created a document-term matrix or equivalent representation suitable for LDA.\n",
    " - Selected appropriate parameters for the LDA model, such as the number of topics and hyperparameters.\n",
    " - Correctly implemented the LDA model, ensuring it is properly trained on the dataset.\n",
    " - Provided a clear and insightful interpretation of the topics, explaining the significance and relevance of the discovered topics.\n",
    "\n",
    "## BertTopic\n",
    " - Selected an appropriate dataset and prepared it for analysis, including cleaning and formatting the data.\n",
    " - Correctly generated text embeddings using a suitable model for input into BERTopic.\n",
    " - Correctly implemented the BERTopic model, ensuring it is properly trained on the dataset.\n",
    " - Accurately extracted and represented topics from the BERTopic model.\n",
    " - Provided a clear and insightful interpretation of the topics, explaining the significance and relevance of the discovered topics.\n",
    "\n",
    "## Decision Tree / Random Forest\n",
    " - Formulated a relevant and appropriate classification objective for the NLP task.\n",
    " - Pre-processed the text data appropriately, including vecterization and other necessary steps.\n",
    " - Properly trained and tested the decision tree or random forest model.\n",
    " - Accurately print or visualize the results, or provide insightful interpretation of the findings.\n",
    "\n",
    "## LLM-based text classification\n",
    " - Formulated a relevant and appropriate classification objective for the NLP task.\n",
    " - Correctly prepared the data for the LLM, ensuring it is suitable for model input.\n",
    " - Properly ran the LLM and tested the LLM output.\n",
    " - Accurately print or visualize the results, or provide insightful interpretation of the findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3-markdown",
   "metadata": {},
   "source": [
    "Pick a dataset of your choice. Please ensure your dataset is a csv file under 100MB named sem3_topic2_nlp_summative_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT modify the contents of this cell. Start your customization in the next one!\n",
    "import pandas as pd\n",
    "\n",
    "custom_data_path = \"sem3_topic2_nlp_summative_data.csv\"\n",
    "custom_df = pd.read_csv(custom_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5646127",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"vertical-align: top; padding-right: 20px;\">\n",
    "\n",
    "<h2>BACKSTORY</h2>\n",
    "\n",
    "<p>\n",
    "Religion has always been an interesting topic for me. My parents never baptised me, choosing instead to let me decide my own beliefs when I was old enough. Still, my mother is strongly Orthodox, so growing up we celebrated Christmas on the night of the 6th to the 7th of January.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Now, living in Amsterdam, I have a Christian boyfriend. This year I will be joining his family for a Catholic Christmas celebration. They are very religious people (typical Lebanese-Italian family), and as a respectful girlfriend I decided to use this summative project as an opportunity to impress my “mother-in-law” with my growing Bible knowledge.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "So in this notebook I will be <b>(1) exploring different topics that emerge throughout Bible verses</b>, and <b>(2) training a model to classify each verse into its discovered topic.\n",
    "</b>\n",
    "\n",
    "<p>\n",
    "the image was generated with ChatGPT\n",
    "</p>\n",
    "\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "    <img src=\"cross-bible.png\" width=\"750\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb54a44",
   "metadata": {},
   "source": [
    "**DEVELOPING A PIPELINE**\n",
    "\n",
    "\n",
    "below I revisit Fromative ipynb 1 to figure out NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0a111",
   "metadata": {},
   "source": [
    "## Preprocessing the text for NLP\n",
    "Preprocessing can involve some combination of the following steps. Which steps to use depends on what you want to do.\n",
    "\n",
    "1. *Remove unwanted or empty messages.* We start by cleaning the data, removing messages that are unlikely to contain any useful text.\n",
    "\n",
    "2. *Text Cleaning.*\n",
    "The first step is to clean the text. We remove any irrelevant items like HTML tags, URLs, and codes when dealing with web data. We also get rid of special characters, numbers, or punctuation that might not be necessary for analysis.\n",
    "\n",
    "3. *Case Normalization.*\n",
    "Next, we normalize the case by converting all the text to lower case. This ensures that words like 'House', 'house', and 'HOUSE' are all treated as the same word, preventing the model from treating them as different entities.\n",
    "\n",
    "4. *Tokenization.*\n",
    "Then we move to tokenization. This is where we break down the text into smaller pieces, or tokens. Tokens can be words, phrases, or even sentences. In English, this might seem as simple as splitting by spaces, but it can get complicated with languages that don’t use spaces or have complex morphology.\n",
    "\n",
    "5. *Stop Words Removal.*\n",
    "After tokenization, we often remove stop words. These are common words like 'is', 'and', 'the', which appear frequently in the text but usually don’t carry significant meaning for the analysis.\n",
    "\n",
    "6. *Lemmatization.*\n",
    "Now, we refine our tokens using ste lemmatization. This strips the words down to their root form. For example, 'running', 'runs', and 'ran' might all be reduced to 'run'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5de1fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#core\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#text preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#LDA topic modelling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#BERTopic\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "#visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#NLTK \n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684eba2",
   "metadata": {},
   "source": [
    "(0) **DATA PREPROCESSING**\n",
    "\n",
    "\n",
    "My original dataset is json so I need to convert into csv to satisfy assingment criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95521ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JSON\n",
    "with open(\"ASV.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for book in data[\"books\"]:\n",
    "    book_name = book[\"name\"]\n",
    "    for chapter in book[\"chapters\"]:\n",
    "        chapter_num = chapter[\"chapter\"]\n",
    "        for verse in chapter[\"verses\"]:\n",
    "            verse_num = verse[\"verse\"]\n",
    "            text = verse[\"text\"]\n",
    "            rows.append({\n",
    "                \"book\": book_name,\n",
    "                \"chapter\": chapter_num,\n",
    "                \"verse\": verse_num,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# IMPORTANT: use quoting to prevent broken columns\n",
    "df.to_csv(\"sem3_topic2_nlp_summative_data.csv\", index=False, quoting=1)  # quoting=1 == csv.QUOTE_ALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2f3852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31102 entries, 0 to 31101\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   book     31102 non-null  object\n",
      " 1   chapter  31102 non-null  int64 \n",
      " 2   verse    31102 non-null  int64 \n",
      " 3   text     31102 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 972.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sem3_topic2_nlp_summative_data.csv\")\n",
    "df.head()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd6736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      book  chapter  verse                                               text\n",
      "0  Genesis        1      1  In the beginning God created the heavens and t...\n",
      "1  Genesis        1      2  And the earth was waste and void; and darkness...\n",
      "2  Genesis        1      3  And God said, Let there be light: and there wa...\n",
      "3  Genesis        1      4  And God saw the light, that it was good: and G...\n",
      "4  Genesis        1      5  And God called the light Day, and the darkness...\n",
      "In the beginning God created the heavens and the earth. \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sem3_topic2_nlp_summative_data.csv\")\n",
    "print(df.head())\n",
    "print(df.iloc[0][\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
